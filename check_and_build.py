#!/usr/bin/env python3
"""
Check and build the vector store if needed
"""

import os
from dotenv import load_dotenv
from modules.scraper import ChangiAirportScraper
from modules.text_splitter import TextProcessor
from modules.rag_pipeline import RAGPipeline

def main():
    """Check and build vector store if needed"""
    print("ğŸ” Checking Changi Airport RAG Chatbot Setup")
    print("=" * 50)
    
    # Load environment variables
    load_dotenv()
    
    # Check API key
    api_key = os.getenv('GOOGLE_API_KEY')
    if not api_key:
        print("âŒ GOOGLE_API_KEY not found in environment variables")
        print("Please set your Google API key in the .env file")
        return False
    
    print("âœ… API key found")
    
    # Check if vector store exists
    db_path = "data/changi_airport_chroma"
    if os.path.exists(db_path):
        print("âœ… Vector store exists")
        
        # Test loading it
        try:
            rag_pipeline = RAGPipeline(api_key)
            if rag_pipeline.load_vector_store():
                print("âœ… Vector store loaded successfully")
                stats = rag_pipeline.get_stats()
                print(f"ğŸ“Š Document count: {stats.get('document_count', 'unknown')}")
                return True
            else:
                print("âš ï¸  Vector store exists but failed to load")
        except Exception as e:
            print(f"âŒ Error loading vector store: {e}")
    else:
        print("âŒ Vector store not found")
    
    # Build the vector store
    print("\nğŸ”¨ Building vector store...")
    try:
        # Initialize components
        scraper = ChangiAirportScraper()
        text_processor = TextProcessor(
            chunk_size=int(os.getenv('CHUNK_SIZE', 1000)),
            chunk_overlap=int(os.getenv('CHUNK_OVERLAP', 200))
        )
        rag_pipeline = RAGPipeline(api_key)
        
        # Scrape websites
        print("ğŸ“¡ Scraping websites...")
        scraped_data = scraper.scrape_all_sites()
        
        if not scraped_data:
            print("âŒ No data scraped from websites")
            return False
        
        print(f"âœ… Scraped {len(scraped_data)} pages")
        
        # Process text into chunks
        print("âœ‚ï¸  Processing text into chunks...")
        chunks = text_processor.process_scraped_data(scraped_data)
        chunks = text_processor.filter_chunks(chunks)
        
        if not chunks:
            print("âŒ No valid chunks created")
            return False
        
        print(f"âœ… Created {len(chunks)} chunks")
        
        # Build vector store
        print("ğŸ—ï¸  Building vector store...")
        rag_pipeline.build_vector_store(chunks)
        
        print("âœ… Vector store built successfully!")
        
        # Test it
        stats = rag_pipeline.get_stats()
        print(f"ğŸ“Š Document count: {stats.get('document_count', 'unknown')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error building vector store: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ Setup completed successfully!")
        print("You can now run: python run.py")
    else:
        print("\nâŒ Setup failed!")
        print("Please check the error messages above.") 